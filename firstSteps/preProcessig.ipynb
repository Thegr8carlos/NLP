{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LIbrerias ocupadadas"
      ],
      "metadata": {
        "id": "T_Uavpk6dI4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuFC5K-f6cSW",
        "outputId": "c1162463-ea04-46d1-b9b2-be351094bd54"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funciones implementadas"
      ],
      "metadata": {
        "id": "Z8kd3Lmi6sCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def read_data(filename, ext):\n",
        "    if ext == 'tsv':\n",
        "        df = pd.read_table(filename)\n",
        "        return df\n",
        "\n",
        "def remove_emojis(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # S√≠mbolos y pictogramas\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # Transporte y s√≠mbolos de mapa\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # Banderas (iOS)\n",
        "        u\"\\U00002700-\\U000027BF\"  # Otros s√≠mbolos diversos\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def remove_non_alphabetic(tokens):\n",
        "    return [word for word in tokens if word.isalpha()]\n",
        "\n",
        "def process_text(text, lemmatizer, stemmer, stop_words):\n",
        "    text = text.lower()\n",
        "    text = remove_emojis(text)\n",
        "    text = remove_punctuation(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = remove_non_alphabetic(tokens)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text\n",
        "\n",
        "def process_data(df):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "    df['processed_text'] = df['tweet'].apply(lambda x: process_text(x, lemmatizer, stemmer, stop_words))\n",
        "    return df\n",
        "\n",
        "def get_MTD(df):\n",
        "  vectorizer = CountVectorizer()\n",
        "  X = vectorizer.fit_transform(df['processed_text'])\n",
        "  tdm_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "  print(tdm_df)\n",
        "  return tdm_df"
      ],
      "metadata": {
        "id": "ohnjPKjI6gkj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading Dataset"
      ],
      "metadata": {
        "id": "1McRG6q3eOfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "El dataset ocupado es de parte de TASS, una competencia que se realiza con los a;os con diferentes task, en particular aqui se ocupara el dataset de la task2 el cual consiste de tres archivos, se hara uso del archivo train.tsv en el cual en la siguiete linea se mostraran sus estadisticas."
      ],
      "metadata": {
        "id": "kVmxfWFOeXGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = read_data(\"train.tsv\", \"tsv\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMqnDabl68SX",
        "outputId": "c24885ef-e53c-482c-cf24-0b683679cc23"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                              tweet    label \n",
            "0   1  El Atl√©tico resignado a perder HASHTAG üòî  http...  sadness \n",
            "1   2  Leer proporciona una mejor visi√≥n del mundo ü§ì ...      joy \n",
            "2   3  Amo a Arya Stark por encima de todas las cosas...      joy \n",
            "3   4  Gracias HASHTAG es incre√≠ble que una ni√±a logr...   others \n",
            "4   5  Solo siento que hayamos perdido 24 esca√±os de ...  sadness \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "x8H45k1HhQQl",
        "outputId": "3529ee24-e2c6-4cab-aa64-45186dfd5eb7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id        5886\n",
              "tweet     5886\n",
              "label     5886\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>5886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tweet</th>\n",
              "      <td>5886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <td>5886</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se cuenta con 5886 ejemplos de tweets, cada uno con su etiquetada asociada"
      ],
      "metadata": {
        "id": "4uiZyZhqhdzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Procesando el dataset"
      ],
      "metadata": {
        "id": "UzD8lbVthxsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = process_data(df)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "62k6I0oe7D9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90ea572-ec00-4099-9ffa-e83282382235"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'tweet', 'label ', 'processed_text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despues de procesado el dataset, se agrega una nueva columna con el texto procesado"
      ],
      "metadata": {
        "id": "Eg8IMy_oiLUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['label ','processed_text']\n",
        "print(df[cols])"
      ],
      "metadata": {
        "id": "3EA2GfMY9s0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fef108-a71e-4925-c8e4-cef54cdd38ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         label                                      processed_text\n",
            "0      sadness   atl√©tico resignado perder hashtag httpstcoaiaa...\n",
            "1          joy   leer proporciona mejor visi√≥n mundo hashtag ha...\n",
            "2          joy   amo arya stark encima toda cosa gameofthron ha...\n",
            "3       others   gracia hashtag incre√≠bl ni√±a logr liderar tan ...\n",
            "4      sadness          solo siento perdido esca√±o cordura hashtag\n",
            "...         ...                                                ...\n",
            "5881    others   peque√±o gesto pued hacer medio ambient hashtag...\n",
            "5882  surprise   do coronacion celebraron hashtag napole√≥n √∫nic...\n",
            "5883     anger   mientra reflexionamo duro medio nueva guerra f...\n",
            "5884   sadness   fachada catedr notr dame salvada podr√° restaur...\n",
            "5885       joy            sufriendo barsa si messi hashtag hashtag\n",
            "\n",
            "[5886 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obteniendo la matriz termino documento"
      ],
      "metadata": {
        "id": "1kAKYpbbkyhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " matrix_term_document = get_MTD(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAEGv_O_ezkZ",
        "outputId": "21bc939f-8f9f-4411-f04e-8005d0a4ffaf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      aaaaaaaaaah  aaaaaahsgajdkahakshah  aaahhhh  abajo  abandona  \\\n",
            "0               0                      0        0      0         0   \n",
            "1               0                      0        0      0         0   \n",
            "2               0                      0        0      0         0   \n",
            "3               0                      0        0      0         0   \n",
            "4               0                      0        0      0         0   \n",
            "...           ...                    ...      ...    ...       ...   \n",
            "5881            0                      0        0      0         0   \n",
            "5882            0                      0        0      0         0   \n",
            "5883            0                      0        0      0         0   \n",
            "5884            0                      0        0      0         0   \n",
            "5885            0                      0        0      0         0   \n",
            "\n",
            "      abandonada  abandonado  abandonar  abandono  abanico  ...  √≥rgano  \\\n",
            "0              0           0          0         0        0  ...       0   \n",
            "1              0           0          0         0        0  ...       0   \n",
            "2              0           0          0         0        0  ...       0   \n",
            "3              0           0          0         0        0  ...       0   \n",
            "4              0           0          0         0        0  ...       0   \n",
            "...          ...         ...        ...       ...      ...  ...     ...   \n",
            "5881           0           0          0         0        0  ...       0   \n",
            "5882           0           0          0         0        0  ...       0   \n",
            "5883           0           0          0         0        0  ...       0   \n",
            "5884           0           0          0         0        0  ...       0   \n",
            "5885           0           0          0         0        0  ...       0   \n",
            "\n",
            "      √∫ltima  √∫ltimo  √∫nase  √∫nete  √∫nica  √∫nicament  √∫nico  √∫nicoooo  √∫til  \n",
            "0          0       0      0      0      0          0      0         0     0  \n",
            "1          0       0      0      0      0          0      0         0     0  \n",
            "2          0       0      0      0      0          0      0         0     0  \n",
            "3          0       0      0      0      0          0      0         0     0  \n",
            "4          0       0      0      0      0          0      0         0     0  \n",
            "...      ...     ...    ...    ...    ...        ...    ...       ...   ...  \n",
            "5881       0       0      0      0      0          0      0         0     0  \n",
            "5882       0       0      0      0      0          0      1         0     0  \n",
            "5883       0       0      0      0      0          0      0         0     0  \n",
            "5884       0       0      0      0      0          0      0         0     0  \n",
            "5885       0       0      0      0      0          0      0         0     0  \n",
            "\n",
            "[5886 rows x 11816 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardando la matriz en un csv"
      ],
      "metadata": {
        "id": "dO2BW3T4k23B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_term_document.to_csv('matriz_termino_documento.csv', index=False)"
      ],
      "metadata": {
        "id": "eiGJAnv9e0pS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cantidad de tokens para el primer documento"
      ],
      "metadata": {
        "id": "J4UalRo_micF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(matrix_term_document.iloc[0].sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5oXlv-DlBvS",
        "outputId": "4f21450b-2044-4592-b50d-fff2e25d3373"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANTES DEL PROCESO"
      ],
      "metadata": {
        "id": "4hqQcArcmpsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.iloc[0].tweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKthQ4EolnLb",
        "outputId": "b64baf51-8888-4398-ce65-8cdf4c3e003b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El Atl√©tico resignado a perder HASHTAG üòî  https://t.co/AiaaGdxmQO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DESPUES DEL PROCESO"
      ],
      "metadata": {
        "id": "D5Fm1k8ymsoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.iloc[0].processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgJXXt-Al4Xn",
        "outputId": "8c562080-227c-4e82-e403-d20cd88badf5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "atl√©tico resignado perder hashtag httpstcoaiaagdxmqo\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}